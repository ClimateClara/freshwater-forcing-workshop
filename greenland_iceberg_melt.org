
#+PROPERTY: header-args:jupyter-python+ :session marson_2024
#+PROPERTY: header-args:bash+ :session (concat "*" (nth 1 (reverse (split-string default-directory "/"))) "-shell*")

* Introduction

** Data

*** Printout

#+BEGIN_SRC bash :exports both :results verbatim
ncdump -h ./dat/GL_iceberg_melt_4326.nc | grep -vE 'coordinates|_Fill|spatial_ref'
#+END_SRC

#+RESULTS:
#+begin_example
netcdf GL_iceberg_melt_4326 {
dimensions:
	lat = 1800 ;
	lon = 3600 ;
variables:
	double NO(lat, lon) ;
	double NW(lat, lon) ;
	double NE(lat, lon) ;
	double CW(lat, lon) ;
	double CE(lat, lon) ;
	double SW(lat, lon) ;
	double SE(lat, lon) ;
	double GL(lat, lon) ;
	double lat(lat) ;
		lat:long_name = "latitude" ;
		lat:units = "degrees_north" ;
	double lon(lon) ;
		lon:long_name = "longitude" ;
		lon:units = "degrees_east" ;
}
#+end_example

#+BEGIN_SRC bash :exports both :results verbatim
ncdump -h ./dat/GL_iceberg_melt_3413.nc | grep -vE 'coordinates|_Fill|spatial_ref'
#+END_SRC

#+RESULTS:
#+begin_example
netcdf GL_iceberg_melt_3413 {
dimensions:
	y = 205 ;
	x = 146 ;
variables:
	double NO(y, x) ;
	double NW(y, x) ;
	double NE(y, x) ;
	double CW(y, x) ;
	double CE(y, x) ;
	double SW(y, x) ;
	double SE(y, x) ;
	double GL(y, x) ;
	double x(x) ;
		x:long_name = "projection_x_coordinate" ;
		x:units = "meters" ;
	double y(y) ;
		y:long_name = "projection_y_coordinate" ;
		y:units = "meters" ;
}
#+end_example

*** Description

=GL_iceberg_melt_4326.nc= contains weighted masks of where iceberg melt enters the ocean. Projection is EPSG:4326 (lon,lat). Spatial resolution is 0.1 degree (1800 x 3600). Fields =NO=, =NW=, =NE=, etc. represent icebergs that are sourced from the North, Northwest, Northeast, etc. sectors of Greenland. The =GL= mask is the sum of all masks. Each mask sums to 1.

#+BEGIN_QUOTE
[!WARNING]
Iceberg tracks come from Marson (2024) http://doi.org/10.1029/2023jc020697. The highest melt rates (largest meltwater injection) occurs in near-coastal cells. If your model has land covering some of these cells, your mask will not sum to 1. Rescaling the mask so it sums to 1 for your ocean is a good idea, but this also redistributes the largest melt points over the entire melt region. It may be better to re-scale the mask by increasing only the largest cell or largest few cells (which are hopefully nearby the coast and the high melt rate cells that were covered by land)
#+END_QUOTE

*** Graphic

The dataset contains one additional array for all of Greenland combined.

[[./fig/GL_berg_melt.png]]

* Processing
** Provenance

Data from Marson (2024) http://doi.org/10.1029/2023jc020697 

+ https://canwin-datahub.ad.umanitoba.ca/data/dataset/nemo-anha4-seaice-locking-icebergs/resource/8aa9c193-214e-4152-9abe-037010bf1999

** Tests and checks

*** Algorithm demonstration

#+BEGIN_SRC jupyter-python :exports both
# synthetic ice mass array, dimesions [x=time, y=mass]
mass = np.zeros((5,5)) * np.nan
mass[0,0] = 5
mass[1,[1,2]] = [10,9]
mass[2,[1,2]] = [10,5]
mass[3,[2,3,4]] = [20,10,5]
mass[4,[3,4]] = [1.1,1]
print('mass', mass)

# flag the first time the iceberg appears
first = np.vstack([np.zeros(mass[:,0].shape)*np.nan, mass.T]).T
first = (~np.isnan(first[:,1:]) & np.isnan(first[:,:-1]))
# print('first', first)

# the water mass is just the derivative of the ice mass in time
h2o = np.vstack([mass.T, np.zeros(mass[:,0].shape)*np.nan]).T
h2o = -1 * np.diff(h2o, axis=1)
# except the diff() drops the last timestep, so we lose that without some extra work...
last = np.vstack([mass.T, np.zeros(mass[:,0].shape)*np.nan]).T
last = (np.isnan(last[:,1:]) & ~np.isnan(last[:,:-1]))
h2o[last] = mass[last]
print('h2o', h2o)

print('ice mass: ', mass[first].sum(), mass[first])
print('water mass: ', np.nansum(h2o), np.nansum(h2o,axis=1))
#+END_SRC

#+RESULTS:
#+begin_example
mass [[ 5.   nan  nan  nan  nan]
 [ nan 10.   9.   nan  nan]
 [ nan 10.   5.   nan  nan]
 [ nan  nan 20.  10.   5. ]
 [ nan  nan  nan  1.1  1. ]]
h2o [[ 5.   nan  nan  nan  nan]
 [ nan  1.   9.   nan  nan]
 [ nan  5.   5.   nan  nan]
 [ nan  nan 10.   5.   5. ]
 [ nan  nan  nan  0.1  1. ]]
ice mass:  46.1 [ 5.  10.  10.  20.   1.1]
water mass:  46.1 [ 5.  10.  10.  20.   1.1]
#+end_example

*** Tests on real data
**** Load a subset

#+BEGIN_SRC jupyter-python :exports both
import xarray as xr
import numpy as np

root = "~/data/Marson_2024/"

mass = xr.open_mfdataset(root+'from_email/mass_01.nc')
bits = xr.open_mfdataset(root+'from_email/mass_of_bits_01.nc')
scale = xr.open_mfdataset(root+'from_email/mass_scaling_01.nc')

# xarray needs things named the same in order to multiply them together.
bits = bits.rename({'mass_of_bits':'mass'})
scale = scale.rename({'mass_scaling':'mass'})

ds = xr.merge([(mass+bits)*scale])
ds = ds.rename({'timestep':'time'})

# %time ds = ds.isel({'particle':np.arange(1000), 'time':np.arange(1000)}).load()

ds['time'].attrs['calendar'] = 'noleap'
ds['time'].attrs['units'] = 'days since 2000-01-01'
ds['time'] = np.arange(ds['time'].values.size).astype(np.int16)
ds['particle'] = ds['particle'].astype(np.int32)

print(ds)
#+END_SRC

#+RESULTS:
: <xarray.Dataset>
: Dimensions:   (time: 5840, particle: 10000)
: Coordinates:
:   * time      (time) int16 0 1 2 3 4 5 6 ... 5833 5834 5835 5836 5837 5838 5839
:   * particle  (particle) int32 117 118 128 129 ... 205888 205896 205897 205916
: Data variables:
:     mass      (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>


**** Initial ice mass should equal final water mass

#+BEGIN_SRC jupyter-python :exports both
# flag the first time the iceberg appears
empty = np.empty(ds['particle'].size)*np.nan
first = np.vstack([empty, ds['mass'].values.T]).T
first = (~np.isnan(first[:,1:]) & np.isnan(first[:,:-1]))

# the water mass is just the derivative of the ice mass in time
h2o = np.vstack([ds['mass'].T, empty]).T
h2o = -1 * np.diff(h2o, axis=1)
# except the diff() drops the last timestep, so we lose that without some extra work...
last = np.vstack([ds['mass'].T, empty]).T
last = (np.isnan(last[:,1:]) & ~np.isnan(last[:,:-1]))
h2o[last] = ds['mass'].values[last]

ds['h2o'] = (('particle','time'), h2o)
print(ds)

print('ice mass: ', ds['mass'].values[first].sum())
print('water mass: ', ds['h2o'].sum().values)
print('diff: ', (ds['mass'].values[first].sum() - ds['h2o'].sum()).values)
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:   (time: 5840, particle: 10000)
Coordinates:
  ,* time      (time) int16 0 1 2 3 4 5 6 ... 5833 5834 5835 5836 5837 5838 5839
  ,* particle  (particle) int32 117 118 128 129 ... 205888 205896 205897 205916
Data variables:
    mass      (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>
    h2o       (particle, time) float64 nan nan nan nan nan ... nan nan nan nan
ice mass:  1886583699309968.5
water mass:  1886583699309959.8
diff:  8.75
#+end_example

Per Marson (2021) http://doi.org/10.1029/2021jc017542

#+BEGIN_QUOTE
The annual mass loss (hereafter referred as discharge) from the Greenland Ice Sheet (GrIS) is currently estimated to be around 1,100 Gt/yr, half of which is attributed to liquid runoff and the other half to solid discharge (Bam- ber et al., 2012, 2018)

Greenland discharge was provided by Bamber et al. (2012) on a 5 Ã— 5 km grid and was remapped to the ANHA4 grid. According to the averages estimated in Bamber et al. (2012), we divided the total discharge into 46% liquid runoff and 54% solid discharge.
#+END_QUOTE

So discharge should be ~1100*0.54 = 594 Gt/yr

** Load data

In addition to loading the public data from Marson (2024) http://doi.org/10.1029/2023jc020697 we need to add in the bergy bits (personal communication). Also, the provided mass is particles (groups of bergs) and needs to be scaled by Martin (2010) http://doi.org/10.1016/j.ocemod.2010.05.001 Table 1 to convert particle mass to ice mass.

#+BEGIN_SRC jupyter-python :exports both
import xarray as xr
import pandas as pd
import numpy as np

root='~/data/Marson_2024/'

lon = xr.open_mfdataset(root+'lon_*.nc', join='override', concat_dim='particle', combine='nested')
lat = xr.open_mfdataset(root+'lat_*.nc', join='override', concat_dim='particle', combine='nested')
mass = xr.open_mfdataset([root+'from_email/mass_01.nc',
                          root+'from_email/mass_02.nc',
                          root+'from_email/mass_03.nc',
                          root+'from_email/mass_04.nc'],
                         join='override', concat_dim='particle', combine='nested')
bits = xr.open_mfdataset(root+'from_email/mass_of_bits_*.nc', join='override', concat_dim='particle', combine='nested')
scale = xr.open_mfdataset(root+'from_email/mass_scaling_*.nc', join='override', concat_dim='particle', combine='nested')

# xarray needs things named the same in order to multiply them together.
bits = bits.rename({'mass_of_bits':'mass'})
scale = scale.rename({'mass_scaling':'mass'})

%time ds = xr.merge([lon,lat,(mass+bits)*scale])

ds = ds.rename({'timestep':'time'})
ds['time'].attrs['calendar'] = 'noleap'
ds['time'].attrs['units'] = 'days since 2000-01-01'
ds['time'] = np.arange(ds['time'].values.size).astype(np.int16)
ds['particle'] = ds['particle'].astype(np.int32)

print(ds)
#+END_SRC

#+RESULTS:
#+begin_example
CPU times: user 4.67 ms, sys: 1.08 ms, total: 5.75 ms
Wall time: 7.45 ms
<xarray.Dataset>
Dimensions:   (time: 5840, particle: 34025)
Coordinates:
  ,* time      (time) int16 0 1 2 3 4 5 6 ... 5833 5834 5835 5836 5837 5838 5839
  ,* particle  (particle) int32 117 118 128 129 ... 1806577 1806831 1807085
Data variables:
    lon       (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>
    lat       (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>
    mass      (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>
#+end_example

*** Compute mass loss

#+BEGIN_SRC jupyter-python :exports both
# flag the first time the iceberg appears
empty = np.empty(ds['particle'].size)*np.nan
first = np.vstack([empty, ds['mass'].values.T]).T
first = (~np.isnan(first[:,1:]) & np.isnan(first[:,:-1]))

# the water mass is just the derivative of the ice mass in time
h2o = np.vstack([ds['mass'].T, empty]).T
h2o = -1 * np.diff(h2o, axis=1)
# except the diff() drops the last timestep, so we lose that without some extra work...
last = np.vstack([ds['mass'].T, empty]).T
last = (np.isnan(last[:,1:]) & ~np.isnan(last[:,:-1]))
h2o[last] = ds['mass'].values[last]

ds['h2o'] = (('particle','time'), h2o)
ds['first'] = (('particle','time'), first)
print(ds)
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:   (time: 5840, particle: 34025)
Coordinates:
  ,* time      (time) int16 0 1 2 3 4 5 6 ... 5833 5834 5835 5836 5837 5838 5839
  ,* particle  (particle) int32 117 118 128 129 ... 1806577 1806831 1807085
Data variables:
    lon       (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>
    lat       (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>
    mass      (particle, time) float64 dask.array<chunksize=(10000, 5840), meta=np.ndarray>
    h2o       (particle, time) float64 nan nan nan nan ... nan nan nan 1.648e+11
    first     (particle, time) bool False False False False ... False False True
#+end_example


*** Save snapshot

#+BEGIN_SRC jupyter-python :exports both
comp = dict(zlib=True, complevel=2)
encoding = {var: comp for var in ds.data_vars}

delayed_obj = ds.to_netcdf('tmp/bergs.nc', encoding=encoding, compute=False)
from dask.diagnostics import ProgressBar
with ProgressBar():
    results = delayed_obj.compute()

# saves as 175 MB file. Takes a few minutes...
#+END_SRC

#+RESULTS:
: [########################################] | 100% Completed | 86.61 s

*** Load snapshot

#+BEGIN_SRC jupyter-python :exports both
import xarray as xr
import numpy as np
import pandas as pd

%time ds = xr.open_dataset('tmp/bergs.nc').load() # load everything into memory
# Takes a while...
#+END_SRC

#+RESULTS:
: CPU times: user 17.1 s, sys: 1min 19s, total: 1min 37s
: Wall time: 1min 38s

**** Test
#+BEGIN_SRC jupyter-python :exports both
%time ice_mass = ds['mass'].values[ds['first'].values].sum()
print('ice mass: ', ice_mass * 1E-12 / 16) # total kg over 16 years -> Gt/yr
%time water_mass = np.nansum(ds['h2o'].values)
print('water mass: ', water_mass * 1E-12 / 16)
#+END_SRC

#+RESULTS:
: CPU times: user 152 ms, sys: 0 ns, total: 152 ms
: Wall time: 150 ms
: ice mass:  407.2388163829433
: CPU times: user 2.29 s, sys: 12 s, total: 14.3 s
: Wall time: 14.5 s
: water mass:  407.2388163829417

The difference between the Marson (2024) http://doi.org/10.1029/2023jc020697 407 Gt/year and the Mankoff (2020) http://doi.org/10.5194/essd-12-1367-2020 ~500 Gt/year (subject to change with each version) is not important. It can represent a lot of things, most likely that Mankoff (2020) is discharge across flux gates upstream from the terminus, so 100 - 407/500 % = 18.6 % is submarine melt, and the remainder is the Marson icebergs.

Additional melting occurs in the fjord and must be handled if the model does not resolve fjords.

This product should be shared as one and several weighted masks that sum to 1, and then users can scale by their own estimated discharge.

** Iceberg meltwater locations
*** Export each particle to file

+ Warning: 34k files generated here.

#+BEGIN_SRC jupyter-python :exports both
from tqdm import tqdm
for p in tqdm(range(ds['particle'].values.size)):
    df = ds.isel({'particle':p})\
           .to_dataframe()\
           .dropna()
    if df.size == 0: continue
    df.index = df.index - df.index[0] + 1
    df[['particle','lon','lat','mass','h2o']]\
        .to_csv(f"./Marson_2024_tmp/{str(p).zfill(5)}.csv", header=None)
#+END_SRC

#+RESULTS:
: 100% 34025/34025 [02:17<00:00, 246.62it/s]


*** Ingest each track and organize by source

**** Set up domain

#+BEGIN_SRC bash :exports both :results verbatim
[[ -e G_3413 ]] || grass -ec EPSG:3413 ./G_3413
grass ./G_3413/PERMANENT
g.mapset -c Marson_2024
export GRASS_OVERWRITE=1
#+END_SRC

**** Load ice ROIs

#+BEGIN_SRC bash :exports both :results verbatim
ogr2ogr ./tmp/Mouginot.gpkg -t_srs "EPSG:3413" ${DATADIR}/Mouginot_2019/Greenland_Basins_PS_v1.4.2.shp
v.import input=./tmp/Mouginot.gpkg output=GL_all
# clean
v.db.droprow input=GL_all where='NAME like "ICE_CAPS_%"' output=nocaps
v.dissolve input=nocaps column=SUBREGION1 output=GL_dirty
v.clean input=GL_dirty tool=rmarea thresh=1000 output=GL

g.region vector=GL_all res=10000
v.to.rast input=GL output=GL use=cat
#+END_SRC

**** Import each track and find closest ice ROI for initial location

#+BEGIN_SRC bash :exports both :results verbatim
# reorder from "cat,id,lon,lat,ice mass,water mass" to lon,lat,water,id,time
cat Marson_2024_tmp/*.csv | awk -F, '{OFS=",";print $3,$4,$6,$2,$1}' > tracks.csv

cat tracks.csv \
  | m.proj -i input=- separator=comma \
  | tr ' ' ',' \
  | v.in.ascii -n input=- output=bergs sep=, \
               columns='x double,y double,water double,id int,time int'

g.region vector=bergs res=25000 -pa
g.region save=iceberg_region

r.mapcalc "x = x()"
r.mapcalc "y = y()"

# Record nearest region at all times, by finding the region nearest the 1st time
v.db.addcolumn map=bergs columns="region VARCHAR(3)"

v.extract input=bergs where='(time == 1)' output=t0
v.distance from=t0 to=GL upload=to_attr to_column=SUBREGION1 column=region
db.select table=t0|head
db.select table=bergs|head

roi=NO # debug
for roi in NO NE SE SW CW NW CE; do
  echo "Processing ROI: ${roi}"
  ids=$(db.select -c sql="select id from t0 where region == '${roi}'")
  ids=$(echo ${ids}| tr ' ' ',')
  db.execute sql="update bergs set region = \"${roi}\" where id in (${ids})"
done

db.select table=bergs | head -n 10 | column -s"|" -t

# convert to raster, binned by melt per cell (a.k.a density or heat or quilt map)
roi=NO # debug
# this loop takes a few minutes per ROI. Could use GNU parallel.
for roi in NO NE SE SW CW NW CE; do
  echo "Processing ROI: ${roi}"
  v.out.ascii input=bergs output=- format=point columns=water where="region == \"${roi}\"" \
    | r.in.xyz input=- z=4 output=${roi} method=sum
  r.colors -g map=${roi} color=viridis
  
  # Convert from kg/16 years to kg/s
  r.mapcalc "${roi} = ${roi} / 16 / 365 / 86400" 
done
#+END_SRC

**** Sanity check: Gt/year/sector

#+BEGIN_SRC bash :exports both :results verbatim
tot=0
for roi in CE CW NE NO NW SE SW; do
  eval $(r.univar -g ${roi})
  # convert from kg/s to Gt/year
  roi_gt=$(echo "${sum} * 86400 * 365 * 10^(-12)" | bc -l)
  echo "${roi}: ${roi_gt}"
  tot=$(echo "${tot} + ${roi_gt}" | bc -l)
done
echo ""
echo "total: " ${tot}
#+END_SRC

#+RESULTS:
: CE: 60.88001865521231664000
: CW: 64.46425864166702496000
: NE: 25.40014168772459318400
: NO: 28.68058742930748950400
: NW: 97.94160451838922336000
: SE: 111.14793375478535664000
: SW: 18.72428341325589532800
: 
: total:  407.23882810034189961600

My estimates of discharge by ROI?

#+BEGIN_SRC jupyter-python :exports both
import xarray as xr
dd = xr.open_dataset('/home/kdm/data/Mankoff_2020/ice/latest/region.nc')\
       .sel({'time':slice('2000-01-01','2019-12-31')})\
       .resample({'time':'YS'})\
       .mean()\
       .mean(dim='time')\
       ['discharge']

print(dd.sum())
dd.to_dataframe()
#+END_SRC

#+RESULTS:
:RESULTS:
: <xarray.DataArray 'discharge' ()>
: array(476.48053387)
| region   |   discharge |
|----------+-------------|
| CE       |     77.8964 |
| CW       |     86.1499 |
| NE       |     25.9822 |
| NO       |     25.329  |
| NW       |    103.127  |
| SE       |    139.048  |
| SW       |     18.9477 |
:END:

**** Graphic

#+BEGIN_SRC bash :exports both :results verbatim
r.colors -g map=NO,NE,SE,SW,CW,NW,CE color=viridis # range all

for roi in NO NE SE SW CE CW NW; do
  # r.colors -e map=${roi} color=viridis # range each (not all)
  # r.colors --q -g map=${roi} color=viridis # range each (not all)
  rm tmp/GL_berg_melt_${roi}.png
  d.mon start=png output=tmp/GL_berg_melt_${roi}.png height=400 width=300 --o
  d.vect --q GL color=gray fill_color=none
  cat=$(db.select -c sql="select cat from GL where SUBREGION1 = \"${roi}\"")
  d.vect --q GL color=gray fill_color=red cats=${cat}
  d.rast --q ${roi} values=1-1E15
  [[ $roi == "CW" ]] && d.legend -l raster=NO range=1,1E15 label_values=1,10,100,1000,50000 title='kg/s' at=66,95,80,85 fontsize=12
  d.mon stop=png
done

roi=all
rm tmp/GL_berg_melt_${roi}.png
d.mon start=png output=tmp/GL_berg_melt_${roi}.png height=400 width=300 --o
d.vect --q GL color=gray fill_color=none
r.mapcalc "roi_ALL = NO+NE+NW+CW+CE+SW+SE"
r.colors -g map=roi_ALL color=viridis
d.rast --q roi_ALL values=1-1E15
# d.legend -l raster=roi_ALL range=1,1E15 label_values=1,10,100,1000,50000 title='kg/s' at=66,95,80,85 fontsize=9
d.mon stop=png

convert +append tmp/GL_berg_melt_{NW,NO,NE,CE}.png ./tmp/row1.png
convert +append tmp/GL_berg_melt_{CW,SW,SE,all}.png ./tmp/row2.png
convert -append tmp/row{1,2}.png ./fig/GL_berg_melt.png
o ./fig/GL_berg_melt.png
#+END_SRC


[[./fig/GL_berg_melt.png]]

** Reproject from 3413 to 4326

+ Reprojecting raster values introduces scaling issues due to EPSG:4326 cell areas
+ Reprojecting vectors and then binning solves this
  
#+BEGIN_SRC bash :exports both :results verbatim
grass ./G_4326/PERMANENT
g.mapset -c Marson_2024

# g.region -pa res=0:15 s=-90 n=90 w=-180 e=180
g.region -pa s=-90 n=90 w=-180 e=180 cols=3600 rows=1800

r.mapcalc "x = x()"
r.mapcalc "y = y()"

v.proj project=G_3413 mapset=Marson_2024 input=bergs output=bergs
db.select table=bergs|head

# convert to raster, binned by melt per cell (a.k.a density or heat or quilt map)
roi=NO # debug
# this loop takes a few minutes per ROI. Could use GNU parallel.
for roi in NO NE SE SW CW NW CE; do
  echo "Processing ROI: ${roi}"
  v.out.ascii input=bergs output=- format=point columns=water where="region == \"${roi}\"" \
    | r.in.xyz input=- z=4 output=${roi} method=sum
  r.colors -g map=${roi} color=viridis
  
  # Convert from kg/16 years to kg/s
  r.mapcalc "${roi} = ${roi} / 16 / 365 / 86400" 
done
#+END_SRC

*** Sanity check: Gt/year/sector

#+BEGIN_SRC bash :exports both :results verbatim
tot=0
for roi in CE CW NE NO NW SE SW; do
  eval $(r.univar -g ${roi})
  # convert from kg/s to Gt/year
  roi_gt=$(echo "${sum} * 86400 * 365 * 10^(-12)" | bc -l)
  echo "${roi}: ${roi_gt}"
  tot=$(echo "${tot} + ${roi_gt}" | bc -l)
done
echo ""
echo "total: " ${tot}
#+END_SRC

#+RESULTS:
: CE: 60.88002141716420976000
: CW: 64.46424022458436800000
: NE: 25.40014088972705308800
: NO: 28.68057180232560158400
: NW: 97.94161052406375120000
: SE: 111.14793509124540096000
: SW: 18.72428329165102948800
: 
: total:  407.23880324076141408000

My estimates of discharge by ROI?

#+BEGIN_SRC jupyter-python :exports both
import xarray as xr
dd = xr.open_dataset('/home/kdm/data/Mankoff_2020/ice/latest/region.nc')\
       .sel({'time':slice('2000-01-01','2019-12-31')})\
       .resample({'time':'YS'})\
       .mean()\
       .mean(dim='time')\
       ['discharge']

print(dd.sum())
dd.to_dataframe()
#+END_SRC

#+RESULTS:
:RESULTS:
: <xarray.DataArray 'discharge' ()> Size: 8B
: array(476.48053387)
| region   |   discharge |
|----------+-------------|
| CE       |     77.8964 |
| CW       |     86.1499 |
| NE       |     25.9822 |
| NO       |     25.329  |
| NW       |    103.127  |
| SE       |    139.048  |
| SW       |     18.9477 |
:END:



** Export to NetCDF

*** Greenland 3413 projection

#+BEGIN_SRC jupyter-python :exports both
import numpy as np
import xarray as xr
import rioxarray as rxr
from tqdm import tqdm

from grass_session import Session
from grass.script import core as gcore
import grass.script as gscript
# import grass.script.setup as gsetup
# import grass python libraries
from grass.pygrass.modules.shortcuts import general as g
from grass.pygrass.modules.shortcuts import raster as r
from grass.pygrass.modules.shortcuts import vector as v
from grass.pygrass.modules.shortcuts import temporal as t
from grass.script import array as garray

S = Session()
S.open(gisdb=".", location="G_3413", mapset="Marson_2024", create_opts=None)
x = garray.array("x")[::-1,:]
y = garray.array("y")[::-1,:]
NO = garray.array("NO")[::-1,:]
da = xr.DataArray(data = NO,
                  dims = ['y','x'],
                  coords = {'x':x[0,:], 'y':y[:,0]})
ds = xr.Dataset({'NO':da})
ds['NW'] = (('y','x'), garray.array("NW")[::-1,:])
ds['NE'] = (('y','x'), garray.array("NE")[::-1,:])
ds['CW'] = (('y','x'), garray.array("CW")[::-1,:])
ds['CE'] = (('y','x'), garray.array("CE")[::-1,:])
ds['SW'] = (('y','x'), garray.array("SW")[::-1,:])
ds['SE'] = (('y','x'), garray.array("SE")[::-1,:])

S.close() # Done with GRASS

# one mask for all of Greenland
ds['GL'] = ds['NO'] + ds['NW'] + ds['NE'] + ds['CW'] \
    + ds['CE'] + ds['SW'] + ds['SE']

# Normalize all masks
for d in ds.data_vars:
    ds[d] = ds[d] / ds[d].sum()
    
ds = ds.where(ds != 0) # 0 to NaN

ds = ds.rio.write_crs('epsg:3413')
ds = ds.rio.set_spatial_dims('x','y')

ds['y'].attrs['long_name'] = 'projection_y_coordinate'
ds['y'].attrs['units'] = 'meters'
ds['x'].attrs['long_name'] = 'projection_x_coordinate'
ds['x'].attrs['units'] = 'meters'

comp = dict(zlib=True, complevel=2) # Internal NetCDF compression
encoding = {var: comp for var in ds.data_vars}

ds.to_netcdf('./dat/GL_iceberg_melt_3413.nc', encoding=encoding)
print(ds)
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:      (y: 205, x: 146)
Coordinates:
  ,* x            (x) float64 -1.688e+06 -1.662e+06 ... 1.912e+06 1.938e+06
  ,* y            (y) float64 -5.612e+06 -5.588e+06 ... -5.375e+05 -5.125e+05
    spatial_ref  int64 0
Data variables:
    NO           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    NW           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    NE           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    CW           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    CE           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    SW           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    SE           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    GL           (y, x) float64 nan nan nan nan nan nan ... nan nan nan nan nan
#+end_example

*** Global 4326 projection

#+BEGIN_SRC jupyter-python :exports both
import numpy as np
import xarray as xr
import rioxarray as rxr
from tqdm import tqdm

from grass_session import Session
from grass.script import core as gcore
import grass.script as gscript
# import grass.script.setup as gsetup
# import grass python libraries
from grass.pygrass.modules.shortcuts import general as g
from grass.pygrass.modules.shortcuts import raster as r
from grass.pygrass.modules.shortcuts import vector as v
from grass.pygrass.modules.shortcuts import temporal as t
from grass.script import array as garray

S = Session()
S.open(gisdb=".", location="G_4326", mapset="Marson_2024", create_opts=None)
lon = garray.array("x")[::-1,:]
lat = garray.array("y")[::-1,:]
NO = garray.array("NO")[::-1,:]
da = xr.DataArray(data = NO,
                  dims = ['lat','lon'],
                  coords = {'lat':lat[:,0], 'lon':lon[0,:]})
ds = xr.Dataset({'NO':da})
ds['NW'] = (('lat','lon'), garray.array("NW")[::-1,:])
ds['NE'] = (('lat','lon'), garray.array("NE")[::-1,:])
ds['CW'] = (('lat','lon'), garray.array("CW")[::-1,:])
ds['CE'] = (('lat','lon'), garray.array("CE")[::-1,:])
ds['SW'] = (('lat','lon'), garray.array("SW")[::-1,:])
ds['SE'] = (('lat','lon'), garray.array("SE")[::-1,:])

S.close() # Done with GRASS

# one mask for all of Greenland
ds['GL'] = ds['NO'] + ds['NW'] + ds['NE'] + ds['CW'] \
    + ds['CE'] + ds['SW'] + ds['SE']

# Normalize all masks
for d in ds.data_vars:
    ds[d] = ds[d] / ds[d].sum()
    
ds = ds.where(ds != 0) # 0 to NaN

ds = ds.rio.write_crs('epsg:4326')
ds = ds.rio.set_spatial_dims('lon','lat')

ds['lat'].attrs['long_name'] = 'latitude'
ds['lat'].attrs['units'] = 'degrees_north'
ds['lon'].attrs['long_name'] = 'longitude'
ds['lon'].attrs['units'] = 'degrees_east'

comp = dict(zlib=True, complevel=2) # Internal NetCDF compression
encoding = {var: comp for var in ds.data_vars}

ds.to_netcdf('./dat/GL_iceberg_melt_4326.nc', encoding=encoding)
print(ds)
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset> Size: 415MB
Dimensions:      (lat: 1800, lon: 3600)
Coordinates:
  ,* lat          (lat) float64 14kB -89.95 -89.85 -89.75 ... 89.75 89.85 89.95
  ,* lon          (lon) float64 29kB -179.9 -179.8 -179.8 ... 179.7 179.8 179.9
    spatial_ref  int64 8B 0
Data variables:
    NO           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
    NW           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
    NE           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
    CW           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
    CE           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
    SW           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
    SE           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
    GL           (lat, lon) float64 52MB nan nan nan nan nan ... nan nan nan nan
#+end_example

* Figure

#+BEGIN_SRC jupyter-python :exports both
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt

fig, axs = plt.subplots(ncols=4, nrows=2)
axs = axs.flatten()

ds = xr.open_dataset('dat/GL_iceberg_melt_3413.nc')

for i,d in enumerate(ds.data_vars):
    if d == 'spatial_ref': continue
    ax = axs[i]
    cbar = False if d != 'GL' else True
    g = np.log10(ds[d]).plot(ax=ax, add_colorbar=cbar)
    ax.set_xlabel(' ')
    ax.set_ylabel(' ')
    if cbar: g.colorbar.set_label('')
#+END_SRC

#+RESULTS:
[[file:./figs_tmp/a10b48d095a859ee4e37d967b1cc3168d883b48f.png]]

